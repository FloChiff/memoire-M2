\chapter{Séminaires de travail}
\fancyhead[LO, RE]{Séminaires de travail}
Dans le cadre de mon stage, il m'a été conseillé par mon encadrante de participer à plusieurs courts séminaires pour me permettre d'approfondir mes connaissances, autant sur le contenu même du stage que sur son aspect technique qui concerne notamment les statistiques textuelles.

\section{\og~Les mots du droit. Lexicographie numérique de \emph{Des délits et des peines} de Cesare Beccaria et ses traductions en Europe~\fg{}}
Organisé sur quatre jours consécutifs (19-22 février 2019), ce séminaire avait pour but de faire un premier travail en lien avec le projet MetaLEX\index{Projet MetaLEX} et de s'intéresser à Beccaria et à son œuvre, qui seront inhérents et importants pour le projet. Il regroupait les responsables du projet à l'\acrshort{ehess}, Falk Bretschneider et Rainer Maria Kiesow et à l'Université de Trêves, Claudine Moulin et Christof Schöch.
Réparti sur les quatre jours, le séminaire organisait des discussions sur le personnage de Beccaria, sur son œuvre et l'impact qu'il a eu, puis sur la place des mots du droit dans l'Europe de l'époque. Nous nous sommes intéressés également aux humanités numériques, à la manière dont elles sont définies et mises en pratique et notamment pour le domaine de la lexicographie. Les deux derniers jours étaient réservés à une initiation à plusieurs logiciels d'analyse de textes, tel que \textsc{juxta commons} qui utilise la collation pour faire apparaître de plusieurs manières les différences entre textes, \textsc{txm}, pour de l'analyse textuelle sous différentes formes ou encore \textsc{catma}, qui sert à l'annotation de textes. On utilise alors un corpus déjà établi d'un chapitre tiré du Beccaria, qui sera la base de mon travail d'étude et d'analyse sur Beccaria avant l'océrisation\index{OCR!ocerisation@océrisation} d'autres chapitres. L'étude de ce chapitre en groupe a permis de mêler le travail avec les logiciels et le sujet du projet.
Le séminaire s'est clos sur une réflexion sur les prochains chapitres qui pourraient être océrisés pour être étudiés ensuite avec les logiciels utilisés et d'autres, pour pousser la réflexion au maximum.

\section{Les statistiques textuelles}
Dans le cadre d'un séminaire du Laboratoire d'excellence de l'\acrlong{obvil} (Labex \acrshort{obvil}) à la maison de la Recherche (16 mai 2019), Florian Cafiero, ancien master de l'école des Chartes, propose une réflexion de deux heures (1h30 + 30 minutes de questions) sur les statistiques textuelles. Le séminaire se déroulait en plusieurs étapes, avec tout d'abord une présentation sur les statistiques, en présentant leur commencement et l'apparition de la loi normale et la loi de Pareto. Ensuite, en se basant tout d'abord sur une analyse mot à mot puis en se concentrant sur du texte à texte, le séminaire consistait à nous présenter le fond des statistiques textuelles et les multiples expériences et lois qui en ont découlé pour analyser, de diverses manières, les contenus d'un texte. Ainsi, Florian Cafiero s'est notamment étendu sur les lois d'Estoup-Zipf et Zipf-Mandelbrodt, qui ont trait au mot, à ses spécificités et à leur fréquence d'apparition dans un texte, élément très utile en statistique textuelles\index{Statistique textuelle}. Par la suite, il a présenté d'autres lois statistiques, à propos d'apparition de mots (loi Heaps), de propagation d'un mot dans un texte (\og clumping \fg{} ou contagion) ou même de l'apparition d'un même mot entre différents textes d'un même corpus ou même auteur (boîte à moustaches). Enfin, il a abordé brièvement l'analyse textuelle texte par texte, avec l'analyse factorielle. Pour conclure, il a répondu à quelques questions, à propos de certaines fonctionnalités de \textsc{txm}, outil de statistiques textuelles (\acrshort{afc}, Spécificités) et à propos de l'efficacité des lois évoquées en fonction des corpus travaillés.

\section{Méthodes et pratique de la statistique textuelle avec R}
Organisé sur une journée complète à l'\acrshort{ehess} (19 juin 2019), l'atelier \acrshort{tal} se divisait en deux parties~: une théorique et une pratique. Il était présenté par Bénédicte Garnier, ingénieure au Service Méthodes Statistiques de l'Institut National des Études Démographiques (INED), spécialisée dans la statistique textuelle\index{Statistique textuelle},  l'analyse exploratoire multivariée et leurs apprentissages, notamment des conseils en méthodologie sur l'utilisation de ces outils. 
La première partie consistait à faire un résumé de ce qu'était la statistique textuelle\index{Statistique textuelle}, faire l'historique de la discipline, présenter les outils à disposition et expliquer les méthodes de travail de la statistique textuelle\index{Statistique textuelle}. L'origine de la statistique textuelle\index{Statistique textuelle} remonte à 1980, avec les premières analyses ; elle a comme particularité d'être en constante évolution~: des outils se développent de plus en plus, avec beaucoup plus d'options, ce qui provoque une obsolescence de beaucoup des ouvrages sur le sujet. Illustrée par divers corpus et les résultats obtenus grâce aux outils, on observe que la statistique textuelle\index{Statistique textuelle} peut faire ressortir un certain nombre d'informations sur un corpus précis, en fonction des données que l'on souhaite recueillir. On étudie les points positifs et négatifs des différents résultats obtenus par la statistique textuelle\index{Statistique textuelle} et les différences qu'il peut y avoir, inhérentes notamment à la longueur des textes. Les résultats ne seront pas les mêmes si on travaille sur un corpus court, avec un nombre limité de données ou si à l'inverse, on travaille avec de longs textes, de plus assez nombreux. Il faudra surtout faire plus de manipulations dans le deuxième cas, pour obtenir tous les résultats voulus. 
Une fois la partie théorique étudiée, nous sommes passés à la pratique, à l'aide du langage \textsc{r}, du logiciel \textsc{rstudio} et du package \emph{R.Temis} qui permettent, à l'aide de scripts, de réaliser les manipulations évoquées plus tôt. Le travail s'est fait à l'aide de deux corpus, un court, tiré d'une étude à propos de l'Europe et un long, qui reprend les vœux présidentiels de 2013 et 2019. À l'aide de ces corpus et de scripts créés pour l'atelier, nous avons ainsi pu travailler sur R Studio et faire apparaître des nuages de mots, des tables de fréquences, des graphes de mots, des tableaux lexicaux, des analyses factorielles, des dictionnaires, etc. Cela permettait alors de voir une grande partie des outils proposés par les logiciels et modules, de travailler avec et d'observer les résultats que l'on pouvait faire apparaître en fonction des corpus à disposition.